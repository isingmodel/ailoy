# RAG with Vector Store

**Retrieval-Augmented Generation (RAG)** is a useful method when you want to use
AI with your own documents. In RAG, the AI model gets extra knowledge from
outside sources, usually stored in something called a vector store. Instead of
depending only on what the model learned during training, RAG finds and adds
related documents at the time you ask a question. This helps the AI give more
accurate, up-to-date, and relevant answers.

In this example, weâ€™ll walk you through a complete RAG workflow â€” how to build a
vector store(`VectorStore`) and integrate it to the `Agent`.

### Initializing a Vector Store

Ailoy simplifies the construction of RAG pipelines through its built-in
`VectorStore` component, which works alongside the `Agent`.

To initialize a vector store:

<CodeTabs>

```python
from ailoy import Runtime
from ailoy.vectorstore import VectorStore

rt = Runtime()
with VectorStore(rt, "BAAI/bge-m3", "faiss") as vs:
    ...
```

```typescript
import { startRuntime, defineVectorStore } from "ailoy-node";

const rt = await startRuntime();
const vs = await defineVectorStore(rt, "BAAI/bge-m3", "faiss");
```

</CodeTabs>

> Ailoy currently supports both
> [**FAISS**](https://github.com/facebookresearch/faiss) and
> [**ChromaDB**](https://www.trychroma.com/) as vector store backends. Refer to
> the official configuration guide for backend-specific options.

> ðŸ’¡ **Note:** At this time, the only supported embedding model is
> [`BAAI/bge-m3`](https://huggingface.co/BAAI/bge-m3). Additional embedding models
> will be supported in future releases.

### Inserting Documents into the Vector Store

You can insert text along with optional metadata into the vector store:

<CodeTabs>

```python
vs.insert(
    "Ailoy is a lightweight library for building AI applications",
    metadata={"topic": "Ailoy"}
)
```

```typescript
await vs.insert({
  document: "Ailoy is a lightweight library for building AI applications",
  metadata: {
    topic: "Ailoy",
  },
});
```

</CodeTabs>

In practice, you should split large documents into smaller chunks before
inserting them. This improves retrieval quality. You may use any text-splitting
tool (e.g.,
[LangChain](https://python.langchain.com/docs/concepts/text_splitters/)), or
utilize Ailoyâ€™s low-level runtime API for text splitting. (See
[Calling Low-Level APIs](./calling-low-level-apis.mdx) for more details.)

### Retrieving Relevant Documents

To retrieve documents similar to a given query:

<CodeTabs>

```python
query = "What is Ailoy?"
items = vs.retrieve(query, top_k=5)
```

```typescript
const query = "What is Ailoy?";
const items = await vs.retrieve(query, 5);
```

</CodeTabs>

This returns a list of `VectorStoreRetrieveItem` instances representing the most
relevant chunks, ranked by similarity. The number of results is controlled via
the `top_k` parameter (default is 5).

### Constructing an Augmented Prompt

Once documents are retrieved, you can construct a context-enriched prompt as
follows:

<CodeTabs>

```python
prompt = f"""
    Based on the provided contexts, try to answer user's question.
    Context: {[item.document for item in items]}
    Question: {query}
"""
```

```typescript
const prompt = `
  Based on the provided contexts, try to answer user' question.
  Context: ${items.map((item) => item.document)}
  Question: ${query}
`;
```

</CodeTabs>

You can then pass this prompt to the agent for inference:

<CodeTabs>

```python
for resp in agent.query(prompt):
    agent.print(resp)
```

```typescript
for await (const resp of agent.query(prompt)) {
  agent.print(resp);
}
```

</CodeTabs>

### Complete Example

<CodeTabs>

```python
from ailoy import Runtime, Agent, VectorStore

# Initialize Runtime

rt = Runtime()

# Initialize Agent and VectorStore

with Agent(rt, "Qwen/Qwen3-8B") as agent, VectorStore(rt, "BAAI/bge-m3", "faiss") as vs:
    # Insert items
    vs.insert(
        "Ailoy is a lightweight library for building AI applications",
        metadata={"topic": "Ailoy"}
    )

    # Search the most relevant items
    query = "What is Ailoy?"
    items = vs.retrieve(query, top_k=5)

    # Augment user query
    prompt = f"""
        Based on the provided contexts, try to answer user's question.
        Context: {[item.document for item in items]}
        Question: {query}
    """

    # Invoke agent
    for resp in agent.query(prompt):
        agent.print(resp)
```

```typescript
import { createRuntime, defineAgent, defineVectorStore } from "ailoy-node";

async function main() {
  // Initialize Runtime
  const rt = await createRuntime();
  // Initialize Agent
  const agent = await defineAgent(rt, "Qwen/Qwen3-8B");
  // Initialize VectorStore
  const vs = await defineVectorStore(rt, "BAAI/bge-m3", "faiss");

  // Insert items
  await vs.insert({
    document: "Ailoy is a lightweight library for building AI applications",
    metadata: { topic: "Ailoy" },
  });

  // Search the most relevant items
  const query = "What is Ailoy?";
  const items = await vs.retrieve(query, 5);

  // Augment user query
  const prompt = `
    Based on the provided contexts, try to answer user' question.
    Context: ${items.map((item) => item.document)}
    Question: ${query}
  `;

  // Invoke agent
  for await (const resp of agent.query(prompt)) {
    agent.print(resp);
  }

  // Delete agent
  await agent.delete();
}
```

</CodeTabs>

{/* prettier-ignore-start */}

:::note
For best results, ensure your documents are chunked semantically (e.g., by paragraphs or sections).
:::

{/* prettier-ignore-end */}
